{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyONuo4ROLw3i4mW9gQ1Shum"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import Layer, Dense, Dropout, Embedding, LayerNormalization, Input, Lambda\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import pandas as pd\n","import re\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","data_path = '/content/drive/My Drive/conversations/Conversation.csv'\n","df = pd.read_csv(data_path)\n","df = df[['question', 'answer']].dropna()\n","\n","def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r\"[^a-zA-Z0-9?.!,¿' ]+\", \" \", text)\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text\n","\n","df['question'] = df['question'].apply(clean_text)\n","df['answer'] = df['answer'].apply(clean_text)\n","df['answer'] = df['answer'].apply(lambda x: '<start> ' + x + ' <end>')\n","\n","tokenizer = Tokenizer(filters='', oov_token='<unk>')\n","tokenizer.fit_on_texts(pd.concat([df['question'], df['answer']]))\n","\n","VOCAB_SIZE = len(tokenizer.word_index) + 1\n","\n","encoder_sequences = tokenizer.texts_to_sequences(df['question'])\n","decoder_sequences = tokenizer.texts_to_sequences(df['answer'])\n","\n","max_len_input = max(len(seq) for seq in encoder_sequences)\n","max_len_target = max(len(seq) for seq in decoder_sequences)\n","\n","encoder_input = pad_sequences(encoder_sequences, maxlen=max_len_input, padding='post')\n","decoder_input = pad_sequences([seq[:-1] for seq in decoder_sequences], maxlen=max_len_target - 1, padding='post')\n","decoder_target = pad_sequences([seq[1:] for seq in decoder_sequences], maxlen=max_len_target - 1, padding='post')\n","\n","enc_train, enc_temp, dec_in_train, dec_in_temp, dec_tar_train, dec_tar_temp = train_test_split(\n","    encoder_input, decoder_input, decoder_target, test_size=0.2, random_state=42)\n","enc_val, enc_test, dec_in_val, dec_in_test, dec_tar_val, dec_tar_test = train_test_split(\n","    enc_temp, dec_in_temp, dec_tar_temp, test_size=0.5, random_state=42)\n","\n","\n","def get_angles(pos, i, d_model):\n","    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n","    return pos * angle_rates\n","\n","def positional_encoding(position, d_model):\n","    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n","                            np.arange(d_model)[np.newaxis, :],\n","                            d_model)\n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","    pos_encoding = angle_rads[np.newaxis, ...]\n","    return tf.cast(pos_encoding, dtype=tf.float32)\n","\n","def create_padding_mask(seq):\n","    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","    return seq[:, tf.newaxis, tf.newaxis, :]\n","\n","def create_look_ahead_mask(size):\n","    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","    return mask\n","\n","def scaled_dot_product_attention(q, k, v, mask):\n","    matmul_qk = tf.matmul(q, k, transpose_b=True)\n","    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","    scaled_logits = matmul_qk / tf.math.sqrt(dk)\n","    if mask is not None:\n","        scaled_logits += (mask * -1e9)\n","    attention_weights = tf.nn.softmax(scaled_logits, axis=-1)\n","    output = tf.matmul(attention_weights, v)\n","    return output, attention_weights\n","\n","class MultiHeadAttention(Layer):\n","    def __init__(self, d_model, num_heads):\n","        super().__init__()\n","        assert d_model % num_heads == 0\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","        self.depth = d_model // num_heads\n","        self.wq = Dense(d_model)\n","        self.wk = Dense(d_model)\n","        self.wv = Dense(d_model)\n","        self.dense = Dense(d_model)\n","\n","    def split_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    def call(self, v, k, q, mask=None):\n","        batch_size = tf.shape(q)[0]\n","        q = self.wq(q)\n","        k = self.wk(k)\n","        v = self.wv(v)\n","        q = self.split_heads(q, batch_size)\n","        k = self.split_heads(k, batch_size)\n","        v = self.split_heads(v, batch_size)\n","        scaled_attention, att_weights = scaled_dot_product_attention(q, k, v, mask)\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n","        output = self.dense(concat_attention)\n","        return output, att_weights\n","\n","class EncoderLayer(Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super().__init__()\n","        self.mha = MultiHeadAttention(d_model, num_heads)\n","        self.ffn = tf.keras.Sequential([\n","            Dense(dff, activation='relu'),\n","            Dense(d_model)\n","        ])\n","        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = Dropout(rate)\n","        self.dropout2 = Dropout(rate)\n","\n","    def call(self, x, training=False, mask=None):\n","        attn_output, _ = self.mha(x, x, x, mask)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(x + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        out2 = self.layernorm2(out1 + ffn_output)\n","        return out2\n","\n","class DecoderLayer(Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super().__init__()\n","        self.mha1 = MultiHeadAttention(d_model, num_heads)\n","        self.mha2 = MultiHeadAttention(d_model, num_heads)\n","        self.ffn = tf.keras.Sequential([\n","            Dense(dff, activation='relu'),\n","            Dense(d_model)\n","        ])\n","        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = Dropout(rate)\n","        self.dropout2 = Dropout(rate)\n","        self.dropout3 = Dropout(rate)\n","\n","    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n","        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n","        attn1 = self.dropout1(attn1, training=training)\n","        out1 = self.layernorm1(attn1 + x)\n","        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n","        attn2 = self.dropout2(attn2, training=training)\n","        out2 = self.layernorm2(attn2 + out1)\n","        ffn_output = self.ffn(out2)\n","        ffn_output = self.dropout3(ffn_output, training=training)\n","        out3 = self.layernorm3(ffn_output + out2)\n","        return out3, attn_weights_block1, attn_weights_block2\n","\n","class Encoder(Layer):\n","    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, max_pos_encoding, rate=0.1):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.embedding = Embedding(vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(max_pos_encoding, d_model)\n","        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","        self.dropout = Dropout(rate)\n","\n","    def call(self, x, training=False, mask=None):\n","        seq_len = tf.shape(x)[1]\n","        x = self.embedding(x)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","        x = self.dropout(x, training=training)\n","        for i in range(self.num_layers):\n","            x = self.enc_layers[i](x, training=training, mask=mask)\n","        return x\n","\n","class Decoder(Layer):\n","    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, max_pos_encoding, rate=0.1):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.embedding = Embedding(vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(max_pos_encoding, d_model)\n","        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","        self.dropout = Dropout(rate)\n","\n","    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n","        seq_len = tf.shape(x)[1]\n","        attention_weights = {}\n","        x = self.embedding(x)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","        x = self.dropout(x, training=training)\n","        for i in range(self.num_layers):\n","            x, block1, block2 = self.dec_layers[i](x, enc_output, training=training,\n","                                                  look_ahead_mask=look_ahead_mask,\n","                                                  padding_mask=padding_mask)\n","            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n","            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n","        return x, attention_weights\n","\n","\n","num_layers = 4\n","d_model = 128\n","dff = 512\n","num_heads = 8\n","dropout_rate = 0.1\n","\n","encoder_inputs = Input(shape=(max_len_input,), name='encoder_inputs')\n","decoder_inputs = Input(shape=(max_len_target - 1,), name='decoder_inputs')\n","\n","enc_padding_mask = Lambda(create_padding_mask)(encoder_inputs)\n","dec_padding_mask = Lambda(create_padding_mask)(encoder_inputs)\n","look_ahead_mask = Lambda(lambda x: create_look_ahead_mask(tf.shape(x)[1]))(decoder_inputs)\n","dec_target_padding_mask = Lambda(create_padding_mask)(decoder_inputs)\n","combined_mask = Lambda(lambda x: tf.maximum(x[0], x[1]))([look_ahead_mask, dec_target_padding_mask])\n","\n","encoder = Encoder(num_layers, d_model, num_heads, dff, VOCAB_SIZE, max_len_input, dropout_rate)\n","decoder = Decoder(num_layers, d_model, num_heads, dff, VOCAB_SIZE, max_len_target - 1, dropout_rate)\n","\n","enc_output = encoder(encoder_inputs, training=True, mask=enc_padding_mask)\n","dec_output, _ = decoder(decoder_inputs, enc_output, training=True, look_ahead_mask=combined_mask, padding_mask=dec_padding_mask)\n","\n","final_dense = Dense(VOCAB_SIZE, activation='softmax')\n","final_output = final_dense(dec_output)\n","\n","transformer = Model(inputs=[encoder_inputs, decoder_inputs], outputs=final_output)\n","transformer.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","transformer.summary()\n","\n","\n","transformer.fit([enc_train, dec_in_train], dec_tar_train,\n","                batch_size=64,\n","                epochs=20,\n","                validation_data=([enc_val, dec_in_val], dec_tar_val))\n","\n","\n","def evaluate(input_seq, max_len=max_len_target - 1):\n","    encoder_padding_mask = create_padding_mask(input_seq)\n","    enc_out = encoder(input_seq, training=False, mask=encoder_padding_mask)\n","\n","    decoder_input_seq = np.array([[tokenizer.word_index['<start>']]])\n","    output = []\n","\n","    for i in range(max_len):\n","        look_ahead_mask_ = create_look_ahead_mask(decoder_input_seq.shape[1])\n","        dec_target_padding_mask_ = create_padding_mask(decoder_input_seq)\n","        combined_mask_ = tf.maximum(look_ahead_mask_, dec_target_padding_mask_)\n","\n","        dec_out, _ = decoder(\n","            decoder_input_seq, enc_out,\n","            training=False,\n","            look_ahead_mask=combined_mask_,\n","            padding_mask=encoder_padding_mask)\n","\n","        predictions = tf.argmax(final_dense(dec_out), axis=-1)\n","        predicted_id = predictions[:, -1].numpy()[0]\n","\n","        if predicted_id == tokenizer.word_index['<end>']:\n","            break\n","\n","        output.append(tokenizer.index_word.get(predicted_id, '<unk>'))\n","        decoder_input_seq = np.append(decoder_input_seq, [[predicted_id]], axis=1)\n","\n","    return ' '.join(output)\n","\n","\n","for i in range(5):\n","    input_seq = enc_test[i:i+1]\n","    print(\"Input:\", df['question'].iloc[i])\n","    print(\"Output:\", evaluate(input_seq))\n","    print('-' * 50)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"dm67RcHQysGA","executionInfo":{"status":"ok","timestamp":1754809554277,"user_tz":240,"elapsed":130716,"user":{"displayName":"Aarush Kumar","userId":"16118659533235147067"}},"outputId":"26627623-4b42-431b-a92a-255613ae40a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional_17\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_17\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda_5 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m19\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda_7 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m)          │          \u001b[38;5;34m0\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda_8 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m20\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ encoder_1 (\u001b[38;5;33mEncoder\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m1,304,192\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n","│                     │                   │            │ lambda_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda_9 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ lambda_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n","│                     │                   │            │ lambda_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda_6 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m19\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ decoder_1 (\u001b[38;5;33mDecoder\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m), │  \u001b[38;5;34m1,569,408\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n","│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m20\u001b[0m,     │            │ encoder_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n","│                     │ \u001b[38;5;34m20\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m,    │            │ lambda_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n","│                     │ \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, │            │ lambda_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","│                     │ \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m),       │            │                   │\n","│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;45mNone\u001b[0m,   │            │                   │\n","│                     │ \u001b[38;5;34m19\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m,    │            │                   │\n","│                     │ \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   │            │                   │\n","│                     │ \u001b[38;5;34m8\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m),     │            │                   │\n","│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m20\u001b[0m,     │            │                   │\n","│                     │ \u001b[38;5;34m20\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m,    │            │                   │\n","│                     │ \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)]        │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_129 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m3993\u001b[0m)  │    \u001b[38;5;34m515,097\u001b[0m │ decoder_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ encoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,304,192</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","│                     │                   │            │ lambda_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n","│                     │                   │            │ lambda_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ decoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,569,408</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>,     │            │ encoder_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,    │            │ lambda_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n","│                     │ <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, │            │ lambda_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>),       │            │                   │\n","│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │            │                   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,    │            │                   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │            │                   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>),     │            │                   │\n","│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>,     │            │                   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,    │            │                   │\n","│                     │ <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)]        │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_129 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3993</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">515,097</span> │ decoder_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,388,697\u001b[0m (12.93 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,388,697</span> (12.93 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,388,697\u001b[0m (12.93 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,388,697</span> (12.93 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 718ms/step - accuracy: 0.5679 - loss: 5.4666 - val_accuracy: 0.6316 - val_loss: 2.9151\n","Epoch 2/20\n","\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.6211 - loss: 2.9484 - val_accuracy: 0.6316 - val_loss: 2.9020\n","Epoch 3/20\n","\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.6274 - loss: 2.8963 - val_accuracy: 0.6316 - val_loss: 2.9072\n","Epoch 4/20\n","\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.6272 - loss: 2.7989 - val_accuracy: 0.6816 - val_loss: 2.2769\n","Epoch 5/20\n","\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.6756 - loss: 2.2703 - val_accuracy: 0.6816 - val_loss: 2.2668\n","Epoch 6/20\n","\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.6749 - loss: 2.2428 - val_accuracy: 0.6915 - val_loss: 2.1823\n","Epoch 7/20\n","\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.6866 - loss: 2.1433 - val_accuracy: 0.6942 - val_loss: 2.1021\n","Epoch 8/20\n","\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.6920 - loss: 2.0349 - val_accuracy: 0.6899 - val_loss: 2.0607\n","Epoch 9/20\n","\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.6995 - loss: 1.9437 - val_accuracy: 0.6945 - val_loss: 2.0404\n","Epoch 10/20\n","\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.7006 - loss: 1.8793 - val_accuracy: 0.6984 - val_loss: 2.0168\n","Epoch 11/20\n","\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.7058 - loss: 1.8128 - val_accuracy: 0.6989 - val_loss: 2.0104\n","Epoch 12/20\n","\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.7085 - loss: 1.7611 - val_accuracy: 0.7009 - val_loss: 2.0036\n","Epoch 13/20\n","\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7149 - loss: 1.6939 - val_accuracy: 0.6997 - val_loss: 2.0207\n","Epoch 14/20\n","\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.7118 - loss: 1.6928 - val_accuracy: 0.7005 - val_loss: 2.0283\n","Epoch 15/20\n","\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.7151 - loss: 1.6464 - val_accuracy: 0.7022 - val_loss: 2.0447\n","Epoch 16/20\n","\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.7156 - loss: 1.6062 - val_accuracy: 0.7022 - val_loss: 2.0292\n","Epoch 17/20\n","\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.7230 - loss: 1.5378 - val_accuracy: 0.7004 - val_loss: 2.0357\n","Epoch 18/20\n","\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.7240 - loss: 1.5053 - val_accuracy: 0.7011 - val_loss: 2.0486\n","Epoch 19/20\n","\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.7245 - loss: 1.4760 - val_accuracy: 0.6997 - val_loss: 2.0672\n","Epoch 20/20\n","\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7278 - loss: 1.4372 - val_accuracy: 0.7007 - val_loss: 2.1079\n","Input: hi, how are you doing?\n","Output: i don't have to go to the bathroom.\n","--------------------------------------------------\n","Input: i'm fine. how about yourself?\n","Output: i don't know.\n","--------------------------------------------------\n","Input: i'm pretty good. thanks for asking.\n","Output: i don't got a house.\n","--------------------------------------------------\n","Input: no problem. so how have you been?\n","Output: and i didn't go to the mouth.\n","--------------------------------------------------\n","Input: i've been great. what about you?\n","Output: i was a lot of money.\n","--------------------------------------------------\n"]}]}]}