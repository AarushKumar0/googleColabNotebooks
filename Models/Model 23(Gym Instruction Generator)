{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPIGvWCxI14KXrWKxClUIWM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P24U2jXwojkI","executionInfo":{"status":"ok","timestamp":1754807528499,"user_tz":240,"elapsed":50979,"user":{"displayName":"Aarush Kumar","userId":"16118659533235147067"}},"outputId":"a6c0b5d6-2d5a-4f22-b701-e2a119efffcb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","data_path = '/content/drive/My Drive/gym/gym_exercise_dataset.csv'"]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Layer, Lambda, Concatenate\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np\n","import re\n","df = pd.read_csv(data_path)"],"metadata":{"id":"23B0aUHgpIXw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import Layer, Dense, Dropout, Embedding, LayerNormalization, Input, Lambda\n","from tensorflow.keras.models import Model\n","import numpy as np\n","import pandas as pd\n","import re\n","from sklearn.model_selection import train_test_split\n","\n","# --- 1️⃣ Data loading and preprocessing ---\n","\n","# Use relevant columns (example)\n","df = df[['Equipment', 'Target_Muscles', 'Force', 'Difficulty (1-5)', 'Preparation', 'Execution', 'Exercise Name']].dropna()\n","\n","# Simple cleaning function\n","def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r\"[^a-zA-Z0-9?.!,¿' ]+\", \" \", text)\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text\n","\n","# Clean text fields to be used as output sequences\n","df['Preparation'] = df['Preparation'].apply(clean_text)\n","df['Execution'] = df['Execution'].apply(clean_text)\n","df['Exercise Name'] = df['Exercise Name'].apply(clean_text)\n","\n","# Combine output columns as one target sentence with <start> and <end> tokens\n","df['target_text'] = '<start> ' + df['Exercise Name'] + ' . ' + df['Preparation'] + ' . ' + df['Execution'] + ' <end>'\n","\n","# Input columns: Equipment, Target_Muscles, Force, Difficulty\n","# Convert categorical to string and join as single input sentence\n","df['input_text'] = df['Equipment'].astype(str) + ' ' + df['Target_Muscles'].astype(str) + ' ' + df['Force'].astype(str) + ' ' + df['Difficulty (1-5)'].astype(str)\n","\n","# Tokenizer for input and output texts\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Fit tokenizer on both inputs and targets combined (shared vocab)\n","tokenizer = Tokenizer(filters='', oov_token='<unk>')\n","tokenizer.fit_on_texts(pd.concat([df['input_text'], df['target_text']]))\n","\n","VOCAB_SIZE = len(tokenizer.word_index) + 1\n","\n","# Convert texts to sequences\n","input_sequences = tokenizer.texts_to_sequences(df['input_text'])\n","target_sequences = tokenizer.texts_to_sequences(df['target_text'])\n","\n","# Max lengths for padding\n","max_len_input = max(len(seq) for seq in input_sequences)\n","max_len_target = max(len(seq) for seq in target_sequences)\n","\n","encoder_input = pad_sequences(input_sequences, maxlen=max_len_input, padding='post')\n","decoder_input = pad_sequences([seq[:-1] for seq in target_sequences], maxlen=max_len_target-1, padding='post')\n","decoder_target = pad_sequences([seq[1:] for seq in target_sequences], maxlen=max_len_target-1, padding='post')\n","\n","# Split train/val/test\n","enc_train, enc_temp, dec_in_train, dec_in_temp, dec_tar_train, dec_tar_temp = train_test_split(\n","    encoder_input, decoder_input, decoder_target, test_size=0.2, random_state=42)\n","enc_val, enc_test, dec_in_val, dec_in_test, dec_tar_val, dec_tar_test = train_test_split(\n","    enc_temp, dec_in_temp, dec_tar_temp, test_size=0.5, random_state=42)\n","\n","# --- 2️⃣ Define Transformer building blocks ---\n","\n","def scaled_dot_product_attention(q, k, v, mask):\n","    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n","    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","    if mask is not None:\n","        scaled_attention_logits += (mask * -1e9)\n","\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n","    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n","\n","    return output, attention_weights\n","\n","class MultiHeadAttention(Layer):\n","    def __init__(self, d_model, num_heads):\n","        super().__init__()\n","        assert d_model % num_heads == 0\n","\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","        self.depth = d_model // num_heads\n","\n","        self.wq = Dense(d_model)\n","        self.wk = Dense(d_model)\n","        self.wv = Dense(d_model)\n","\n","        self.dense = Dense(d_model)\n","\n","    def split_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0,2,1,3])  # (batch_size, num_heads, seq_len, depth)\n","\n","    def call(self, v, k, q, mask=None):\n","        batch_size = tf.shape(q)[0]\n","\n","        q = self.wq(q)  # (batch_size, seq_len_q, d_model)\n","        k = self.wk(k)\n","        v = self.wv(v)\n","\n","        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n","        k = self.split_heads(k, batch_size)\n","        v = self.split_heads(v, batch_size)\n","\n","        scaled_attention, att_weights = scaled_dot_product_attention(q, k, v, mask)\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n","\n","        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n","\n","        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n","\n","        return output, att_weights\n","\n","class EncoderLayer(Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super().__init__()\n","\n","        self.mha = MultiHeadAttention(d_model, num_heads)\n","        self.ffn = tf.keras.Sequential([\n","            Dense(dff, activation='relu'),\n","            Dense(d_model)\n","        ])\n","\n","        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = Dropout(rate)\n","        self.dropout2 = Dropout(rate)\n","\n","    def call(self, x, training=False, mask=None):\n","        attn_output, _ = self.mha(x, x, x, mask)  # Self attention\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(x + attn_output)  # Residual connection\n","\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        out2 = self.layernorm2(out1 + ffn_output)\n","\n","        return out2\n","\n","class DecoderLayer(Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super().__init__()\n","\n","        self.mha1 = MultiHeadAttention(d_model, num_heads)\n","        self.mha2 = MultiHeadAttention(d_model, num_heads)\n","\n","        self.ffn = tf.keras.Sequential([\n","            Dense(dff, activation='relu'),\n","            Dense(d_model)\n","        ])\n","\n","        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = Dropout(rate)\n","        self.dropout2 = Dropout(rate)\n","        self.dropout3 = Dropout(rate)\n","\n","    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n","        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # Self-attention\n","        attn1 = self.dropout1(attn1, training=training)\n","        out1 = self.layernorm1(attn1 + x)\n","\n","        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)  # Encoder-decoder attention\n","        attn2 = self.dropout2(attn2, training=training)\n","        out2 = self.layernorm2(attn2 + out1)\n","\n","        ffn_output = self.ffn(out2)\n","        ffn_output = self.dropout3(ffn_output, training=training)\n","        out3 = self.layernorm3(ffn_output + out2)\n","\n","        return out3, attn_weights_block1, attn_weights_block2\n","\n","class Encoder(Layer):\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n","        super().__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = Embedding(input_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n","\n","        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","\n","        self.dropout = Dropout(rate)\n","\n","    def call(self, x, training=False, mask=None):\n","        seq_len = tf.shape(x)[1]\n","\n","        x = self.embedding(x)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","\n","        x = self.dropout(x, training=training)\n","\n","        for i in range(self.num_layers):\n","            x = self.enc_layers[i](x, training=training, mask=mask)\n","\n","        return x\n","\n","class Decoder(Layer):\n","    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n","        super().__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = Embedding(target_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n","\n","        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","        self.dropout = Dropout(rate)\n","\n","    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n","        seq_len = tf.shape(x)[1]\n","        attention_weights = {}\n","\n","        x = self.embedding(x)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","\n","        x = self.dropout(x, training=training)\n","\n","        for i in range(self.num_layers):\n","            x, block1, block2 = self.dec_layers[i](x, enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=padding_mask)\n","\n","            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n","            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n","\n","        return x, attention_weights\n","\n","def positional_encoding(position, d_model):\n","    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n","                            np.arange(d_model)[np.newaxis, :],\n","                            d_model)\n","\n","    # apply sin to even indices in the array\n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","\n","    # apply cos to odd indices in the array\n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","\n","    pos_encoding = angle_rads[np.newaxis, ...]\n","\n","    return tf.cast(pos_encoding, dtype=tf.float32)\n","\n","def get_angles(pos, i, d_model):\n","    angle_rates = 1 / np.power(10000, (2*(i//2)) / np.float32(d_model))\n","    return pos * angle_rates\n","\n","def create_padding_mask(seq):\n","    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","    # add extra dimensions to add the padding to the attention logits.\n","    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n","\n","def create_look_ahead_mask(size):\n","    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","    return mask  # (seq_len, seq_len)\n","\n","# --- 3️⃣ Build the full Transformer model ---\n","\n","num_layers = 4\n","d_model = 128\n","dff = 512\n","num_heads = 8\n","dropout_rate = 0.1\n","\n","encoder_inputs = Input(shape=(max_len_input,), name=\"encoder_inputs\")\n","decoder_inputs = Input(shape=(max_len_target-1,), name=\"decoder_inputs\")\n","\n","enc_padding_mask = Lambda(create_padding_mask)(encoder_inputs)\n","dec_padding_mask = Lambda(create_padding_mask)(encoder_inputs)\n","look_ahead_mask = Lambda(lambda x: create_look_ahead_mask(tf.shape(x)[1]))(decoder_inputs)\n","dec_target_padding_mask = Lambda(create_padding_mask)(decoder_inputs)\n","combined_mask = Lambda(lambda x: tf.maximum(x[0], x[1]))([look_ahead_mask, dec_target_padding_mask])\n","\n","encoder = Encoder(num_layers, d_model, num_heads, dff, VOCAB_SIZE, max_len_input, dropout_rate)\n","decoder = Decoder(num_layers, d_model, num_heads, dff, VOCAB_SIZE, max_len_target-1, dropout_rate)\n","\n","enc_output = encoder(encoder_inputs, training=True, mask=enc_padding_mask)\n","dec_output, _ = decoder(decoder_inputs, enc_output, training=True, look_ahead_mask=combined_mask, padding_mask=dec_padding_mask)\n","\n","final_output = Dense(VOCAB_SIZE, activation='softmax')(dec_output)\n","\n","transformer = Model(inputs=[encoder_inputs, decoder_inputs], outputs=final_output)\n","\n","transformer.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","transformer.summary()\n","\n","# --- 4️⃣ Train ---\n","\n","transformer.fit([enc_train, dec_in_train], dec_tar_train,\n","                batch_size=64,\n","                epochs=50,\n","                validation_data=([enc_val, dec_in_val], dec_tar_val))\n","\n","# --- Save final Dense layer separately for inference ---\n","final_dense_layer = transformer.layers[-1]\n","\n","# --- 5️⃣ Inference functions (simplified greedy decode) ---\n","\n","def evaluate(input_seq, max_len=max_len_target-1):\n","    encoder_input_seq = input_seq\n","    encoder_padding_mask = create_padding_mask(encoder_input_seq)\n","\n","    enc_out = encoder(encoder_input_seq, training=False, mask=encoder_padding_mask)\n","\n","    decoder_input_seq = np.array([[tokenizer.word_index['<start>']]])\n","    output = []\n","\n","    for i in range(max_len):\n","        look_ahead_mask_ = create_look_ahead_mask(decoder_input_seq.shape[1])\n","        dec_target_padding_mask_ = create_padding_mask(decoder_input_seq)\n","        combined_mask_ = tf.maximum(look_ahead_mask_, dec_target_padding_mask_)\n","\n","        dec_out, _ = decoder(\n","            decoder_input_seq, enc_out,\n","            training=False,\n","            look_ahead_mask=combined_mask_,\n","            padding_mask=encoder_padding_mask\n","        )\n","\n","        predictions = tf.argmax(final_dense_layer(dec_out), axis=-1)\n","        predicted_id = predictions[:, -1].numpy()[0]\n","\n","        if predicted_id == tokenizer.word_index['<end>']:\n","            break\n","\n","        output.append(tokenizer.index_word.get(predicted_id, '<unk>'))\n","        decoder_input_seq = np.append(decoder_input_seq, [[predicted_id]], axis=1)\n","\n","    return ' '.join(output)\n","\n","# --- 6️⃣ Test sample ---\n","\n","sample_idx = 0\n","print(\"Input:\", df.iloc[sample_idx]['input_text'])\n","print(\"Output:\", evaluate(np.array([encoder_input[sample_idx]])))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"eAAKiK_yuU7n","executionInfo":{"status":"ok","timestamp":1754808938726,"user_tz":240,"elapsed":127401,"user":{"displayName":"Aarush Kumar","userId":"16118659533235147067"}},"outputId":"a05a3eea-2c8a-4214-ce4d-b348374ee914"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional_30\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_30\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m135\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda_21 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m12\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda_23 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;34m135\u001b[0m, \u001b[38;5;34m135\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda_24 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m135\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ encoder_4 (\u001b[38;5;33mEncoder\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m938,624\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n","│                     │                   │            │ lambda_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda_25 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m135\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ lambda_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n","│                     │ \u001b[38;5;34m135\u001b[0m)              │            │ lambda_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda_22 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m12\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ decoder_4 (\u001b[38;5;33mDecoder\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m135\u001b[0m,      │  \u001b[38;5;34m1,203,840\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n","│                     │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m,   │            │ encoder_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n","│                     │ \u001b[38;5;34m135\u001b[0m, \u001b[38;5;34m135\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, │            │ lambda_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n","│                     │ \u001b[38;5;34m8\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m),     │            │ lambda_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m135\u001b[0m,    │            │                   │\n","│                     │ \u001b[38;5;34m135\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m,   │            │                   │\n","│                     │ \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, │            │                   │\n","│                     │ \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m135\u001b[0m, \u001b[38;5;34m135\u001b[0m),     │            │                   │\n","│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;45mNone\u001b[0m,   │            │                   │\n","│                     │ \u001b[38;5;34m12\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m,    │            │                   │\n","│                     │ \u001b[38;5;34m135\u001b[0m, \u001b[38;5;34m135\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, │            │                   │\n","│                     │ \u001b[38;5;34m8\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)]     │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_297 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m135\u001b[0m, \u001b[38;5;34m1137\u001b[0m) │    \u001b[38;5;34m146,673\u001b[0m │ decoder_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">135</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">135</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">135</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">135</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ encoder_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">938,624</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","│                     │                   │            │ lambda_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">135</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">135</span>)              │            │ lambda_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lambda_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ decoder_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">135</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,203,840</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,   │            │ encoder_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">135</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">135</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, │            │ lambda_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>),     │            │ lambda_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">135</span>,    │            │                   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">135</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,   │            │                   │\n","│                     │ <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, │            │                   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">135</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">135</span>),     │            │                   │\n","│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │            │                   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,    │            │                   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">135</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">135</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, │            │                   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)]     │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_297 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">135</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1137</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">146,673</span> │ decoder_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,289,137\u001b[0m (8.73 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,289,137</span> (8.73 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,289,137\u001b[0m (8.73 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,289,137</span> (8.73 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 4s/step - accuracy: 0.3799 - loss: 5.5181 - val_accuracy: 0.5609 - val_loss: 3.7175\n","Epoch 2/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.5602 - loss: 3.6204 - val_accuracy: 0.5609 - val_loss: 3.2546\n","Epoch 3/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.5605 - loss: 3.2150 - val_accuracy: 0.5609 - val_loss: 3.1574\n","Epoch 4/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 0.5556 - loss: 3.1590 - val_accuracy: 0.5609 - val_loss: 3.1337\n","Epoch 5/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 0.5605 - loss: 3.1093 - val_accuracy: 0.5609 - val_loss: 3.1304\n","Epoch 6/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.5655 - loss: 3.0689 - val_accuracy: 0.5609 - val_loss: 3.1160\n","Epoch 7/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.5556 - loss: 3.0998 - val_accuracy: 0.5609 - val_loss: 2.9050\n","Epoch 8/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.5688 - loss: 2.7382 - val_accuracy: 0.5609 - val_loss: 2.5819\n","Epoch 9/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.5618 - loss: 2.5719 - val_accuracy: 0.5753 - val_loss: 2.4968\n","Epoch 10/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.5755 - loss: 2.4572 - val_accuracy: 0.5774 - val_loss: 2.4580\n","Epoch 11/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 0.5734 - loss: 2.4505 - val_accuracy: 0.5830 - val_loss: 2.4365\n","Epoch 12/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.5815 - loss: 2.3957 - val_accuracy: 0.5834 - val_loss: 2.4372\n","Epoch 13/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.5807 - loss: 2.3993 - val_accuracy: 0.5834 - val_loss: 2.4125\n","Epoch 14/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.5731 - loss: 2.4171 - val_accuracy: 0.5827 - val_loss: 2.3640\n","Epoch 15/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.5841 - loss: 2.3145 - val_accuracy: 0.5775 - val_loss: 2.3204\n","Epoch 16/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.5830 - loss: 2.2797 - val_accuracy: 0.5827 - val_loss: 2.2821\n","Epoch 17/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.5818 - loss: 2.2672 - val_accuracy: 0.5908 - val_loss: 2.2238\n","Epoch 18/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.5920 - loss: 2.2027 - val_accuracy: 0.5993 - val_loss: 2.1787\n","Epoch 19/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.6053 - loss: 2.1145 - val_accuracy: 0.6096 - val_loss: 2.0986\n","Epoch 20/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.6208 - loss: 2.0293 - val_accuracy: 0.6312 - val_loss: 2.0176\n","Epoch 21/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.6404 - loss: 1.9358 - val_accuracy: 0.6502 - val_loss: 1.9014\n","Epoch 22/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.6494 - loss: 1.8983 - val_accuracy: 0.6710 - val_loss: 1.7712\n","Epoch 23/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.6823 - loss: 1.6990 - val_accuracy: 0.6846 - val_loss: 1.6724\n","Epoch 24/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.6990 - loss: 1.5823 - val_accuracy: 0.6974 - val_loss: 1.5894\n","Epoch 25/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.7082 - loss: 1.5076 - val_accuracy: 0.7086 - val_loss: 1.5168\n","Epoch 26/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.7222 - loss: 1.4078 - val_accuracy: 0.7141 - val_loss: 1.4462\n","Epoch 27/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.7243 - loss: 1.3710 - val_accuracy: 0.7235 - val_loss: 1.3913\n","Epoch 28/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.7391 - loss: 1.2824 - val_accuracy: 0.7326 - val_loss: 1.3402\n","Epoch 29/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.7513 - loss: 1.2087 - val_accuracy: 0.7425 - val_loss: 1.2936\n","Epoch 30/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.7602 - loss: 1.1567 - val_accuracy: 0.7493 - val_loss: 1.2557\n","Epoch 31/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.7742 - loss: 1.0819 - val_accuracy: 0.7532 - val_loss: 1.2264\n","Epoch 32/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.7762 - loss: 1.0632 - val_accuracy: 0.7628 - val_loss: 1.1820\n","Epoch 33/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.7937 - loss: 0.9775 - val_accuracy: 0.7669 - val_loss: 1.1545\n","Epoch 34/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.7960 - loss: 0.9628 - val_accuracy: 0.7735 - val_loss: 1.1237\n","Epoch 35/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.8110 - loss: 0.8807 - val_accuracy: 0.7772 - val_loss: 1.0938\n","Epoch 36/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.8151 - loss: 0.8545 - val_accuracy: 0.7855 - val_loss: 1.0601\n","Epoch 37/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.8247 - loss: 0.8099 - val_accuracy: 0.7870 - val_loss: 1.0376\n","Epoch 38/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.8325 - loss: 0.7665 - val_accuracy: 0.7944 - val_loss: 1.0139\n","Epoch 39/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.8353 - loss: 0.7472 - val_accuracy: 0.7955 - val_loss: 1.0083\n","Epoch 40/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8422 - loss: 0.7135 - val_accuracy: 0.8000 - val_loss: 0.9829\n","Epoch 41/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.8509 - loss: 0.6715 - val_accuracy: 0.8074 - val_loss: 0.9618\n","Epoch 42/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8566 - loss: 0.6426 - val_accuracy: 0.8092 - val_loss: 0.9457\n","Epoch 43/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.8637 - loss: 0.6107 - val_accuracy: 0.8086 - val_loss: 0.9297\n","Epoch 44/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8680 - loss: 0.5823 - val_accuracy: 0.8184 - val_loss: 0.9169\n","Epoch 45/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8772 - loss: 0.5419 - val_accuracy: 0.8171 - val_loss: 0.9064\n","Epoch 46/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8784 - loss: 0.5453 - val_accuracy: 0.8197 - val_loss: 0.8976\n","Epoch 47/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.8856 - loss: 0.5064 - val_accuracy: 0.8257 - val_loss: 0.8835\n","Epoch 48/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8834 - loss: 0.5121 - val_accuracy: 0.8280 - val_loss: 0.8748\n","Epoch 49/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8948 - loss: 0.4610 - val_accuracy: 0.8296 - val_loss: 0.8586\n","Epoch 50/50\n","\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.8981 - loss: 0.4476 - val_accuracy: 0.8296 - val_loss: 0.8545\n","Input: cable sternocleidomastoid, pull 2\n","Output: seated calf raise . place shoulders under padded lever. place feet shoulder width apart on platform. grasp handles to each side of platform. place toes and balls of feet on support lever. . raise heels by extending ankles as far as possible. lower heels by bending ankles until calves are stretched. repeat.\n"]}]}]}