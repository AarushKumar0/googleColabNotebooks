{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28462,"status":"ok","timestamp":1755398555901,"user":{"displayName":"Aarush Kumar","userId":"16118659533235147067"},"user_tz":240},"id":"JP3724Og5opy","outputId":"37e32128-822e-4324-ec6b-89fd4e064950"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install -q trimesh shapely rtree scikit-image pandas\n","\n","import os, glob, random, math, time\n","import numpy as np\n","import pandas as pd\n","import trimesh\n","from skimage.measure import marching_cubes\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":495},"executionInfo":{"elapsed":410975,"status":"error","timestamp":1755399004694,"user":{"displayName":"Aarush Kumar","userId":"16118659533235147067"},"user_tz":240},"id":"3xuKjI466ASk","outputId":"a421ba83-8b73-4ca7-e2b9-c10f6cf421fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Device: cuda\n","Train shapes: 9843 | Test shapes: 2468\n"]},{"ename":"RuntimeError","evalue":"DataLoader worker (pid(s) 9965) exited unexpectedly","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/signal_handling.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 9965) is killed by signal: Killed. ","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1513177227.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mrunning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0mpartial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B,1,D,H,W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mfull\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1410\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m   1265\u001b[0m                     \u001b[0;34mf\"DataLoader worker (pid(s) {pids_str}) exited unexpectedly\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m                 ) from e\n","\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 9965) exited unexpectedly"]}],"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install -q trimesh shapely rtree scikit-image pandas\n","\n","import os, glob, random, math, time\n","import numpy as np\n","import pandas as pd\n","import trimesh\n","from skimage.measure import marching_cubes\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","MODELNET_ROOT = '/content/drive/My Drive/3DObjectReconstruction/ModelNet40'\n","CSV_PATH      = '/content/drive/My Drive/3DObjectReconstruction/metadata_modelnet40.csv'\n","VOXEL_RES     = 32\n","BATCH_SIZE    = 8\n","EPOCHS        = 30\n","LR           = 2e-3\n","NUM_WORKERS   = 2\n","SAVE_DIR      = '/content/modelnet_shape_completion_out'\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Device:', device)\n","\n","\n","def load_off(path):\n","    \"\"\"\n","    Load OFF mesh with trimesh, center to centroid and scale to fit unit cube.\n","    Handles scenes by merging into a single Trimesh.\n","    \"\"\"\n","    mesh = trimesh.load(path, force='mesh')\n","    if not isinstance(mesh, trimesh.Trimesh):\n","        mesh = trimesh.util.concatenate(mesh.dump())\n","    mesh = mesh.copy()\n","    mesh.apply_translation(-mesh.centroid)\n","    max_extent = mesh.bounding_box.extents.max() + 1e-8\n","    mesh.apply_scale(1.0 / max_extent)\n","    return mesh\n","\n","def mesh_to_vox(mesh, res=VOXEL_RES):\n","    \"\"\"\n","    Convert mesh to occupancy grid using trimesh voxelization.\n","    Pads/crops to exactly (res,res,res).\n","    Returns np.uint8 array in {0,1} with shape (D,H,W) aka (Z,Y,X).\n","    \"\"\"\n","    pitch = 1.0 / res\n","    vg = mesh.voxelized(pitch=pitch)  # VoxelGrid\n","    occ = vg.matrix.astype(np.uint8)  # (Z,Y,X)\n","\n","    D,H,W = occ.shape\n","    pad_d = max(0, res - D)\n","    pad_h = max(0, res - H)\n","    pad_w = max(0, res - W)\n","    if pad_d or pad_h or pad_w:\n","        occ = np.pad(occ, ((0,pad_d),(0,pad_h),(0,pad_w)), mode='constant', constant_values=0)\n","    occ = occ[:res, :res, :res]\n","    return occ\n","\n","\n","def random_block_mask(occ, min_ratio=0.2, max_ratio=0.5):\n","    D,H,W = occ.shape\n","    dr = random.uniform(min_ratio, max_ratio)\n","    hr = random.uniform(min_ratio, max_ratio)\n","    wr = random.uniform(min_ratio, max_ratio)\n","    d = max(1, int(D*dr)); h = max(1, int(H*hr)); w = max(1, int(W*wr))\n","    z0 = random.randint(0, D-d); y0 = random.randint(0, H-h); x0 = random.randint(0, W-w)\n","    out = occ.copy()\n","    out[z0:z0+d, y0:y0+h, x0:x0+w] = 0\n","    return out\n","\n","def random_point_dropout(occ, drop_prob=0.3):\n","    out = occ.copy()\n","    mask = (np.random.rand(*occ.shape) < drop_prob) & (occ == 1)\n","    out[mask] = 0\n","    return out\n","\n","def make_partial(occ, mode='mix'):\n","    if mode == 'block': return random_block_mask(occ)\n","    if mode == 'drop':  return random_point_dropout(occ)\n","    # mix\n","    return random_block_mask(occ) if random.random() < 0.5 else random_point_dropout(occ)\n","\n","# 4) Dataset using your CSV\n","class ModelNetCSVVoxels(Dataset):\n","    \"\"\"\n","    Expects objects.csv with: object_id,class,split,object_path\n","    object_path is relative to MODELNET_ROOT, e.g. 'airplane/test/airplane_0627.off'\n","    \"\"\"\n","    def __init__(self, root, csv_path, split='train', res=VOXEL_RES, corrupt=True):\n","        self.root = root\n","        self.df = pd.read_csv(csv_path)\n","        self.df = self.df[self.df['split'].str.lower() == split.lower()].reset_index(drop=True)\n","        self.res = res\n","        self.corrupt = corrupt\n","\n","        # Pre-build file list\n","        self.files = []\n","        for _, row in self.df.iterrows():\n","            rel = str(row['object_path']).lstrip('/\\\\')\n","            full = os.path.join(root, rel)\n","            self.files.append(full)\n","\n","    def __len__(self): return len(self.files)\n","\n","    def __getitem__(self, idx):\n","        path = self.files[idx]\n","        # robust loading (skip corrupt files)\n","        try:\n","            mesh = load_off(path)\n","            occ = mesh_to_vox(mesh, res=self.res).astype(np.float32)   # (D,H,W) in {0,1}\n","        except Exception as e:\n","            # In case of read error, return blank volume pair to keep training robust\n","            occ = np.zeros((self.res, self.res, self.res), dtype=np.float32)\n","\n","        full = occ\n","        partial = make_partial(occ) if self.corrupt else occ\n","        # (1,D,H,W) channels-first for 3D convs\n","        full    = np.expand_dims(full, 0)\n","        partial = np.expand_dims(partial, 0)\n","        return torch.from_numpy(partial), torch.from_numpy(full)\n","\n","# 5) Model â€” fully custom (no Sequential), explicit layers & forward\n","class Conv3DAE(nn.Module):\n","    def __init__(self, base=24):\n","        super().__init__()\n","        # Encoder (32 -> 16 -> 8 -> 4)\n","        self.conv1 = nn.Conv3d(1, base, 3, padding=1)\n","        self.bn1   = nn.BatchNorm3d(base)\n","\n","        self.conv2 = nn.Conv3d(base, base, 3, padding=1, stride=2)  # 32->16\n","        self.bn2   = nn.BatchNorm3d(base)\n","\n","        self.conv3 = nn.Conv3d(base, base*2, 3, padding=1)\n","        self.bn3   = nn.BatchNorm3d(base*2)\n","\n","        self.conv4 = nn.Conv3d(base*2, base*2, 3, padding=1, stride=2)  # 16->8\n","        self.bn4   = nn.BatchNorm3d(base*2)\n","\n","        self.conv5 = nn.Conv3d(base*2, base*4, 3, padding=1)\n","        self.bn5   = nn.BatchNorm3d(base*4)\n","\n","        self.conv6 = nn.Conv3d(base*4, base*4, 3, padding=1, stride=2)  # 8->4\n","        self.bn6   = nn.BatchNorm3d(base*4)\n","\n","        # Decoder (4 -> 8 -> 16 -> 32)\n","        self.deconv1 = nn.ConvTranspose3d(base*4, base*2, 4, stride=2, padding=1)  # 4->8\n","        self.dbn1    = nn.BatchNorm3d(base*2)\n","        self.dconv1  = nn.Conv3d(base*2, base*2, 3, padding=1)\n","        self.dbn1b   = nn.BatchNorm3d(base*2)\n","\n","        self.deconv2 = nn.ConvTranspose3d(base*2, base, 4, stride=2, padding=1)    # 8->16\n","        self.dbn2    = nn.BatchNorm3d(base)\n","        self.dconv2  = nn.Conv3d(base, base, 3, padding=1)\n","        self.dbn2b   = nn.BatchNorm3d(base)\n","\n","        self.deconv3 = nn.ConvTranspose3d(base, base, 4, stride=2, padding=1)      # 16->32\n","        self.dbn3    = nn.BatchNorm3d(base)\n","\n","        self.out_conv = nn.Conv3d(base, 1, 1)  # logits\n","\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        # Encoder\n","        x = self.relu(self.bn1(self.conv1(x)))\n","        x = self.relu(self.bn2(self.conv2(x)))\n","        x = self.relu(self.bn3(self.conv3(x)))\n","        x = self.relu(self.bn4(self.conv4(x)))\n","        x = self.relu(self.bn5(self.conv5(x)))\n","        x = self.relu(self.bn6(self.conv6(x)))\n","        # Decoder\n","        x = self.relu(self.dbn1(self.deconv1(x)))\n","        x = self.relu(self.dbn1b(self.dconv1(x)))\n","        x = self.relu(self.dbn2(self.deconv2(x)))\n","        x = self.relu(self.dbn2b(self.dconv2(x)))\n","        x = self.relu(self.dbn3(self.deconv3(x)))\n","        x = self.out_conv(x)   # logits\n","        return x\n","\n","# 6) Dataloaders\n","train_set = ModelNetCSVVoxels(MODELNET_ROOT, CSV_PATH, split='train', res=VOXEL_RES, corrupt=True)\n","test_set  = ModelNetCSVVoxels(MODELNET_ROOT, CSV_PATH, split='test',  res=VOXEL_RES, corrupt=True)  # corrupt test for completion\n","\n","print(f\"Train shapes: {len(train_set)} | Test shapes: {len(test_set)}\")\n","\n","train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True,\n","                          num_workers=NUM_WORKERS, pin_memory=True, drop_last=False)\n","test_loader  = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False,\n","                          num_workers=NUM_WORKERS, pin_memory=True, drop_last=False)\n","\n","# 7) Training utils\n","def iou_from_logits(logits, target, thresh=0.4):\n","    prob = torch.sigmoid(logits)\n","    pred = (prob > thresh).float()\n","    inter = (pred * target).sum(dim=(1,2,3,4))\n","    union = ((pred + target) > 0).float().sum(dim=(1,2,3,4))\n","    return ((inter / (union + 1e-6))).mean().item()\n","\n","model = Conv3DAE(base=24).to(device)\n","opt = torch.optim.Adam(model.parameters(), lr=LR)\n","criterion = nn.BCEWithLogitsLoss()\n","\n","best_iou = 0.0\n","for epoch in range(1, EPOCHS+1):\n","    model.train()\n","    running = []\n","    for partial, full in train_loader:\n","        partial = partial.to(device)  # (B,1,D,H,W)\n","        full    = full.to(device)\n","        opt.zero_grad()\n","        logits = model(partial)\n","        loss = criterion(logits, full)\n","        loss.backward()\n","        opt.step()\n","        running.append(loss.item())\n","    tr_loss = float(np.mean(running)) if running else 0.0\n","\n","    # quick eval\n","    model.eval()\n","    with torch.no_grad():\n","        v_losses, v_ious = [], []\n","        for i, (partial, full) in enumerate(test_loader):\n","            partial = partial.to(device); full = full.to(device)\n","            logits = model(partial)\n","            v_losses.append(criterion(logits, full).item())\n","            v_ious.append(iou_from_logits(logits, full, thresh=0.4))\n","            if i >= 4:  # small subset for speed\n","                break\n","        val_loss = float(np.mean(v_losses)) if v_losses else 0.0\n","        val_iou  = float(np.mean(v_ious)) if v_ious else 0.0\n","\n","    print(f\"[{epoch:02d}/{EPOCHS}] train_loss={tr_loss:.4f}  val_loss={val_loss:.4f}  val_iou@0.4={val_iou:.4f}\")\n","\n","    if val_iou > best_iou:\n","        best_iou = val_iou\n","        torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'best_ae.pth'))\n","\n","# 8) Save a few reconstructions as meshes (PLY via marching cubes)\n","def save_mesh_from_volume(vol3d, out_path, threshold=0.4):\n","    \"\"\"\n","    vol3d: numpy (D,H,W) in [0,1] or logits after sigmoid.\n","    Exports ASCII PLY with marching cubes surface.\n","    \"\"\"\n","    vol = np.asarray(vol3d, dtype=np.float32)\n","    verts, faces, _, _ = marching_cubes(vol, level=threshold)\n","    with open(out_path, 'w') as f:\n","        f.write(\"ply\\nformat ascii 1.0\\n\")\n","        f.write(f\"element vertex {len(verts)}\\n\")\n","        f.write(\"property float x\\nproperty float y\\nproperty float z\\n\")\n","        f.write(f\"element face {len(faces)}\\n\")\n","        f.write(\"property list uchar int vertex_indices\\nend_header\\n\")\n","        for v in verts:\n","            f.write(f\"{v[0]} {v[1]} {v[2]}\\n\")\n","        for fc in faces:\n","            f.write(f\"3 {fc[0]} {fc[1]} {fc[2]}\\n\")\n","\n","# dump some samples\n","model.load_state_dict(torch.load(os.path.join(SAVE_DIR, 'best_ae.pth'), map_location=device))\n","model.eval()\n","os.makedirs(os.path.join(SAVE_DIR, 'samples'), exist_ok=True)\n","\n","with torch.no_grad():\n","    saved = 0\n","    for partial, full in test_loader:\n","        partial = partial.to(device)[:1]\n","        full    = full.to(device)[:1]\n","        logits  = model(partial)\n","        pred    = torch.sigmoid(logits)[0,0].cpu().numpy()\n","        tgt     = full[0,0].cpu().numpy()\n","\n","        save_mesh_from_volume(pred, os.path.join(SAVE_DIR, 'samples', f'pred_{saved:03d}.ply'), threshold=0.4)\n","        save_mesh_from_volume(tgt,  os.path.join(SAVE_DIR, 'samples', f'target_{saved:03d}.ply'), threshold=0.5)\n","        saved += 1\n","        if saved >= 10:\n","            break\n","\n","print(\"Done. Outputs saved to:\", SAVE_DIR)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyP4bH6gXqv4YBlbjbfbwjaJ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}